"""
tools : Outils pour le traitement des donn√©es

Objectif :
-----------
Le module vise √† fournir un ensemble d'outils pour faciliter le traitement des donn√©es,
depuis l'acquisition par le biais du web crawling jusqu'√† la manipulation et la mise en forme
des dataframes.
"""

from bs4 import BeautifulSoup
import requests  
import pandas as pd 
from tqdm import tqdm
import re 
import numpy as np

class Crawler :

    """ [Class Docstring]
    Crawler: Classe d'extraction et d'analyse de contenu HTML.

    Cette classe est con√ßue pour extraire le contenu HTML d'une URL sp√©cifi√©e et fournir 
    des m√©thodes pour explorer et extraire des informations sp√©cifiques du contenu.

    Attributs:
    ----------
    Aucun

    M√©thodes:
    ---------
    - extract_html(url: str) -> Union[BeautifulSoup, Dict[str, str]]:
        Effectue une requ√™te HTTP GET √† l'URL sp√©cifi√©e et renvoie le contenu HTML ou un 
        message d'erreur.
    
    - find_elements(html_content: BeautifulSoup, tag: str, class_name: Optional[str], 
                    element_id: Optional[str]) -> Tuple[str, Union[List[BeautifulSoup], None]]:
        Recherche tous les √©l√©ments HTML correspondants dans le contenu fourni en fonction 
        du tag, du nom de classe ou de l'ID et renvoie un tuple contenant un message et la 
        liste des √©l√©ments correspondants.

    - find_element(html_content: BeautifulSoup, tag: str, class_name: Optional[str], 
                   element_id: Optional[str]) -> Union[BeautifulSoup, Dict[str, str]]:
        Recherche le premier √©l√©ment HTML correspondant dans le contenu fourni et renvoie 
        l'√©l√©ment ou un message d'erreur.

    - extract_table(html_content: BeautifulSoup, class_name: Optional[str], table_id: Optional[str]) -> pd.DataFrame:
        Extrait une table du contenu HTML fourni, la convertit en un DataFrame pandas et 
        renvoie le DataFrame.

    Exemple d'utilisation:
    ----------------------
    crawler = Crawler()
    html_content = crawler.extract_html("https://www.example.com")
    message, elements = crawler.find_elements(html_content, 'table', class_name='myTable')
    """

    @staticmethod
    def extract_html(url):

        """Param√®tres:
        -----------
        url : str
            L'URL du site web dont vous souhaitez extraire le contenu HTML.

        Retourne:
        --------
        BeautifulSoup object or dict
            Si la requ√™te est r√©ussie (code HTTP 200), renvoie un objet BeautifulSoup contenant le contenu HTML de la page.
            Si la requ√™te √©choue, renvoie un dictionnaire contenant une cl√© "error" et un message d'erreur d√©taill√©.

        Exceptions:
        ----------
        ValueError: Si l'URL fournie n'est pas de type 'string'.
        requests.RequestException: Si une exception li√©e √† la requ√™te se produit.
        """

        if not isinstance(url, str) : 
            raise ValueError("L'url saisie doit √™tre de type 'string'")

        try:
            response = requests.get(url)

            # Si la requ√™te est r√©ussie (code 200), on renvoie le contenu HTML
            if response.status_code == 200:
                return BeautifulSoup(response.text, 'html.parser')

            # Sinon, on renvoie un dictionnaire d'erreur
            return {"error": f"error in get method: {response.status_code}"}
        
        # Gestion des exceptions li√©es aux requ√™tes
        except requests.RequestException as e:
            return {"error": f"An error occurred: {str(e)}"}
            
    @staticmethod
    def find(html_content, tag,  method = 0 , attrs_key = None, attrs_value = None):

        """ 
        Recherche tous les √©l√©ments HTML dans le contenu fourni en utilisant le tag et des attributs optionnels.

        Param√®tres:
        -----------
        html_content : BeautifulSoup
            Objet BeautifulSoup contenant le contenu HTML √† analyser.
        tag : str
            Tag HTML de l'√©l√©ment √† rechercher (par exemple, 'table', 'div', 'a', etc.).
        method : int, optional (default: 0)
            M√©thode de recherche √† utiliser: 0 pour "find_all" et 1 pour "find".
        attrs_key : str, optional
            Cl√© de l'attribut √† utiliser pour la recherche (par exemple, 'class', 'id', etc.).
        attrs_value : str, optional
            Valeur de l'attribut √† utiliser pour la recherche.

        Retourne:
        --------
        list ou BeautifulSoup object
            Liste des √©l√©ments trouv√©s ou un seul objet BeautifulSoup selon la m√©thode utilis√©e. 
            Si aucun √©l√©ment n'est trouv√©, renvoie une liste vide (pour "find_all") ou None (pour "find").

        Exceptions:
        ----------
        ValueError: Si l'objet html_content fourni n'est pas une instance de BeautifulSoup, 
        si le tag n'est pas une cha√Æne de caract√®res, si method n'est pas 0 ou 1, 
        ou si seulement l'une des valeurs attrs_key ou attrs_value est fournie.
        """

        # V√©rification du type de l'objet html_content
        if not isinstance(html_content, BeautifulSoup): 
            raise ValueError("L'entr√©e doit √™tre un objet BeautifulSoup.")
        
        if not isinstance(tag, str) : 
            raise ValueError("Le tag doit √™tre de type 'string'.") 

        if method not in [0,1] : 
                raise ValueError("Veuillez saisir une m√©thode √©gale √† 0 (find_all) ou 1 (find).")
        
        if attrs_key is not None : 
            if not isinstance(attrs_key, str) : 
                raise ValueError("attrs_key doit √™tre de type 'string'.") 
        if attrs_value is not None : 
            if not isinstance(attrs_value, str) : 
                raise ValueError("attrs_value doit √™tre de type 'string'.") 
        
        if (attrs_key is None  and attrs_value is not None) or (attrs_key is not None  and attrs_value is None) : 
            raise ValueError ("Les 2 arguements attrs_key et attrs_value doivent √™tre renseign√©s.")

        if method == 0 : 
            if attrs_key is not None : 
                res = html_content.find_all(tag, attrs= {attrs_key : attrs_value})
            else : 
                res = html_content.find_all(tag)
        else  :
            if attrs_key is not None : 
                res = html_content.find(tag, attrs= {attrs_key : attrs_value})
            else : 
                res = html_content.find(tag)

        return res 
    
    @staticmethod  
    def extract_table(html_content, class_name=None, table_id=None):

        """ [Function Docstring]
        Extrait une table du contenu HTML fourni en utilisant soit 
        le nom de la classe `class_name` soit l'ID `table_id`.

        Param√®tres:
        -----------
        html_content : BeautifulSoup
            Objet BeautifulSoup contenant le contenu HTML √† analyser.
        class_name : str, optional
            Nom de la classe CSS de la table √† extraire. 
            Utilis√© pour la s√©lection de la table bas√©e sur la classe.
        table_id : str, optional
            ID de la table √† extraire. Utilis√© pour la s√©lection de la table bas√©e sur l'ID.

        Retourne:
        --------
        pd.DataFrame
            DataFrame contenant les donn√©es de la table extraites.

        Exceptions:
        ----------
        ValueError: Si l'objet `html_content` fourni n'est pas une instance de BeautifulSoup, 
        si la table n'est pas trouv√©e, ou si ni `class_name` ni `table_id` ne sont fournis.
        """

        if not isinstance(html_content, BeautifulSoup): 
            raise ValueError("L'entr√©e doit √™tre un objet BeautifulSoup.")
        
        # Extrait la table en utilisant le nom de classe ou l'id
        if class_name is not None: 
            table = html_content.find('table', class_ = class_name)
        elif table_id is not None: 
            table = html_content.find('table', id=table_id)
        else:
            raise ValueError("L'un des deux param√®tres class_name ou table_id doit √™tre fourni.")
        
        # S'assure que la table est trouv√©e
        if table is None:
            raise ValueError("Table non trouv√©e.")
        
        # Extrait l'en-t√™te
        header = [th.text.strip() for th in table.find_all('th')]
            
        # Extrait les donn√©es des lignes
        rows_data = []
        for row in table.find_all('tr'):  # Exclut la ligne d'en-t√™te
            rows_data.append([td.text.strip() for td in row.find_all('td')])
                
        # Convertit en DataFrame
        df = pd.DataFrame(rows_data, columns=header)
        return df

class CleanDf : 

    @staticmethod
    def convert_str_na_to_nan(df, na_values):
        """
        Convertit les cha√Ænes de caract√®res repr√©sentant des NA en NaN.

        Param√®tres:
        - df (pd.DataFrame): Le DataFrame en entr√©e.
        - na_values (list of str): Liste des cha√Ænes de caract√®res repr√©sentant des NA.

        Retourne:
        pd.DataFrame: DataFrame avec les cha√Ænes repr√©sentant des NA converties en NaN.

        Exemple d'utilisation:
        convert_str_na_to_nan(df, na_values=["NA", "N/A", "null"])
        """
        return df.replace(na_values, pd.NA)

    @staticmethod
    def drop_na(df, col=None, cols=None):

        if not isinstance(df, pd.DataFrame):
            raise ValueError("L'entr√©e doit √™tre un DataFrame Pandas.")

        # V√©rifie si le param√®tre `col` est utilis√©.
        if col is not None:
            if col not in df.columns:
                raise ValueError(f"Colonne {col} non trouv√©e dans le DataFrame.")
            return df.dropna(subset=[col])

        # V√©rifie si le param√®tre `cols` est utilis√©.
        elif cols is not None:
            # V√©rifie que cols est une liste
            if not isinstance(cols, list):
                raise ValueError("L'argument `cols` doit √™tre une liste de noms de colonnes.")

            # V√©rifie que chaque √©l√©ment de cols est une cha√Æne et qu'il existe dans le DataFrame
            for col_name in cols:
                if not isinstance(col_name, str):
                    raise ValueError("Tous les √©l√©ments de `cols` doivent √™tre des cha√Ænes repr√©sentant les noms de colonnes.")
                if col_name not in df.columns:
                    raise ValueError(f"Colonne {col_name} non trouv√©e dans le DataFrame.")
            
            return df.dropna(subset=cols)
        
        # Si ni `col` ni `cols` n'est sp√©cifi√©, supprime les lignes avec NA dans toutes les colonnes.
        else:
            return df.dropna()
    
    @staticmethod
    def transformer_observation(obs):

        observations = re.findall(r'(\w+)\n(\w+)', obs)
        resultats = [f"{prenom} {nom}" for prenom, nom in observations]
        return ", ".join(resultats)
        
class FetchData :

    @staticmethod
    def fetch_cpu_data(url, missing_table, session):
        try :
            res = Crawler.extract_html(url)
            table = Crawler.find(res, tag='table', element_id='test-suite-results')
            data = {}
            for row in table.findAll('tr'):
                header = row.find('th').text
                value = row.find('td').text
                data[header] = value
            return data 
        except: 
            data = missing_table
            return data 
        
    def fetch_casto_data (id , session ) : 

        BASE = 'https://www.castorama.fr'
        URL = "https://api.kingfisher.com/prod/v1/product-search-bff/products/CAFR"

        cat_id = id['id']
        content_loc = id['path']
        r = BASE + content_loc

        querystring = {"channelApiVersion":"v2",
                    "filter[category]": cat_id  ,
                    "include":"content",
                    "page[number]":"1",
                    "page[size]":"200",
                    "supportMerchTiles":"true"}

        headers = {
            "cookie": "TS013aa2d6=011543659ba2f180aea3ed106a302f1177feaae80664b510fa08066096b8a6e9edb39cb9bc49da26709b5751a09e0ccbb7c368e320; TSce5a380a027=08016f2e84ab20007f3441d17e1ec166d17880b09688c8d79947b5133c07d36fc64a59724a589171087092cc8a11300066c97d9a8a320061bfef6350baf5a906fa5cb6be47f585bec634bdbec859dffabbd7b5eabc9121a128f8026a58a11da5",
            "Accept": "application/json, text/plain, */*",
            "Origin": "https://www.castorama.fr",
            "Authorization": "Atmosphere atmosphere_app_id=kingfisher-o4ITR0sWAyCVQBraQf4Es61jHV3dN4oO9UwJQMrS",
            "Referer": "https://www.castorama.fr/",
            "Accept-Language": "fr-FR,fr;q=0.9",
            "Host": "api.kingfisher.com",
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "X-Context-Location": f"{content_loc}",
            "x-dtc": f'sn="v_4_srv_-2D64_sn_DT6PV4C3187S2UCTP5RUK787IAUHIMNI", pc="-64$471313258_896h15vHGQIWCKVDUTMAQDCTATKMDWCORMFABRW-0e0", v="16748137103710K7F1MT3VILA0RAK43A6TIH7G93PT88T",  app="7fad07df8aa3fcc7", r= {r}'
        }

        response = session.request("GET", URL, headers=headers, params=querystring)
        data = response.json()

        return data
    
    def fetch_f1_data (race, missing_table, session ) : 

        df_clean = CleanDf()
        try : 

            res = Crawler.extract_html(race)
            table = Crawler.extract_table(res , class_name='resultsarchive-table')

            # Extraire l'ann√©e
            year_match = re.search(r"\d{4}", race)
            table['Season'] = int(year_match.group()) if year_match else None

            # Extraire le nom du Grand Prix (gp_name)
            gp_name_match = re.search(r"/([^/]+)/race-result.html$", race)
            table['Localisation'] = gp_name_match.group(1) if gp_name_match else None

            table = table.iloc[1:-1]
            table['Driver'] = table['Driver'].apply(df_clean.transformer_observation) 

        except : 

            table = missing_table
            year_match = re.search(r"\d{4}", race)
            table['Season'] = int(year_match.group()) if year_match else None
            gp_name_match = re.search(r"/([^/]+)/race-result.html$", race)
            table['Localisation'] = gp_name_match.group(1) if gp_name_match else None
        
        return table
            
class MissingTable : 

    @staticmethod
    def missing_cpu_table (url): 

        res = Crawler.extract_html(url)
        table = Crawler.find(res, tag='table', method=1,
                             attrs_key='id', attrs_value='test-suite-results')

        missing_table = {}
        for row in table.findAll('tr'):
            header = row.find('th').text
            missing_table[header] = "NA"
        
        return missing_table
    
    def f1_missing_table(valid_url) : 

        res = Crawler.extract_html(valid_url)

        valid_table = Crawler.extract_table(res , class_name='resultsarchive-table')
        valid_table.iloc[0, :] = np.nan

        missing_table = valid_table.head(1)
        missing_table = missing_table.iloc[1:-1]

        return missing_table
            
def tqdm_executor_map(executor, function, *args, **kwargs):
    """
    Une fonction pour ex√©cuter une fonction de mani√®re parall√®le 
    tout en affichant une barre de progression avec tqdm.

    Arguments:
    - executor : Un ex√©cuteur de type concurrent.futures 
    (ThreadPoolExecutor ou ProcessPoolExecutor).
    - function : La fonction √† ex√©cuter en parall√®le.
    - *args : Les arguments √† passer √† la fonction.
    - **kwargs : Les arguments cl√©/valeur √† passer √† la fonction.

    Retourne :
    - Une liste des r√©sultats renvoy√©s par la fonction.
    """

    # D√©finition des codes de couleur pour la barre de progression.
    DARK_GREEN = "\033[32m"
    RED = "\033[91m"
    BLUE = "\033[94m"
    ENDC = "\033[0m"
    
    # Format personnalis√© pour tqdm en utlisant les codes couleurs 
    bar_format = "{l_bar}" + DARK_GREEN + "‚ñà{bar:25}‚ñë" + ENDC + " " + RED + "{n_fmt}/{total_fmt}" + ENDC + " " + BLUE + "[{rate_fmt} eta {remaining}]" + ENDC

    # Utilise l'ex√©cuteur pour ex√©cuter la fonction en parall√®le.
    gen = executor.map(function, *args)
    
    # Si un total est fourni dans kwargs, il est utilis√©. Sinon, il est laiss√© √† None.
    total = kwargs.get('total', None)
    
    # Enveloppe le g√©n√©rateur avec tqdm pour afficher la barre de progression.
    return list(tqdm(gen, 
                    desc="Processing data üöÄ ", 
                    bar_format=bar_format,
                    dynamic_ncols=True, 
                    mininterval=0.25,
                    total=total))
